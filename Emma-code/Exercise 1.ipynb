{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1\n",
    " <p><div class=\"lev1\"><a href=\"#Task-A.-Another-LEGO-brick-in-the-wall\"><span class=\"toc-item-num\">Task A.&nbsp;&nbsp;</span>Another LEGO brick in the wall</a></div>\n",
    " <p><div class=\"lev1\"><a href=\"#Task-B.-Drop-the-Bike\"><span class=\"toc-item-num\">Task B.&nbsp;&nbsp;</span>Drop the Bike</a></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your imports here\n",
    "# Importing pandas and numpy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task A. Another LEGO brick in the wall\n",
    "\n",
    "LEGO is a popular brand of toy building bricks. They are often sold in sets in order to build a specific object. Each set contains a number of parts in different shapes, sizes and colors. This database contains information on which parts are included in different LEGO sets. It was originally compiled to help people who owned some LEGO sets already figure out what other sets they could build with the pieces they had.\n",
    "\n",
    "This dataset contains the official LEGO colors, parts, inventories (i.e., sets of LEGO parts which assembled create an object in the LEGO world) and sets (i.e., sets of LEGO inventories which assembled create a LEGO ecosystem). The schema of the dataset can be shown in the following UML diagram: \n",
    "\n",
    "![lego-schema](lego-schema.png)\n",
    "\n",
    "In this task you have to apply the following Data Wrangling pipeline:\n",
    "1. Load your data into `Pandas`\n",
    "* Explore it and clean its dirty parts\n",
    "* Use it to answer a set of queries\n",
    "\n",
    "Each of these subtasks are described in detail below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A1. Loading phase\n",
    "Load all the csv files into different `DataFrames`. Use meaningful names for your `DataFrames` (e.g., the respective filenames).\n",
    "\n",
    "*Hint: You can load files without first unzipping them (for `Pandas` version >= 0.18.1).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEGO_DATA_FOLDER = DATA_FOLDER + '/lego'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "colors = pd.read_csv(LEGO_DATA_FOLDER + \"/colors.csv.zip\")\n",
    "inventories = pd.read_csv(LEGO_DATA_FOLDER + \"/inventories.csv.zip\")\n",
    "inventory_parts = pd.read_csv(LEGO_DATA_FOLDER + \"/inventory_parts.csv.zip\")\n",
    "inventory_sets = pd.read_csv(LEGO_DATA_FOLDER + \"/inventory_sets.csv.zip\")\n",
    "part_categories = pd.read_csv(LEGO_DATA_FOLDER + \"/part_categories.csv.zip\")\n",
    "parts = pd.read_csv(LEGO_DATA_FOLDER + \"/parts.csv.zip\")\n",
    "sets = pd.read_csv(LEGO_DATA_FOLDER + \"/sets.csv.zip\")\n",
    "themes = pd.read_csv(LEGO_DATA_FOLDER + \"/themes.csv.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A2. Cleaning phase\n",
    "Explore the following columns from your dataset:\n",
    "\n",
    "1. sets: year\n",
    "* inventory_parts: quantity\n",
    "\n",
    "What is the time range of the sets? \n",
    "What is the average quantity of the inventory parts? \n",
    "Do you see any inconsistencies? \n",
    "Provide code that detects and cleans such inconsistencies and validates the coherence of your dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanCheck(data):\n",
    "    # Check whether any set has an undefined year\n",
    "    is_nan = data.isna().sum() \n",
    "\n",
    "    # Take out the data that is incorrect\n",
    "    is_bad = data.apply(lambda x: len(x) != 4).sum()\n",
    "    \n",
    "    print(is_nan)\n",
    "    print(is_bad)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3085\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cleanCheck' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-865d5df009dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mclean_sets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mcleanCheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_sets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myear\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cleanCheck' is not defined"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "# Cleaning sets.year \n",
    "\n",
    "# Count number of inconsistencies in the set\n",
    "sets_bad = sets[sets['year'].apply(lambda x: len(x) != 4)]\n",
    "print(sets_bad.year.count())\n",
    "\n",
    "# Make all negative years positive\n",
    "sets['year'] = sets['year'].apply(lambda x: x[1:] if x[0] == \"-\" else x)\n",
    "\n",
    "# Delete last number in years with 5 digits\n",
    "sets['year'] = sets['year'].apply(lambda x: x[:-1] if len(x) == 5 else x)\n",
    "\n",
    "# Change 70s to nan\n",
    "sets['year'] = sets['year'].apply(lambda x: np.nan if len(x) == 3 else x)\n",
    "\n",
    "clean_sets = sets.dropna(axis = 0, subset=['year'])\n",
    "cleanCheck(clean_sets.year)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1        1978\n",
       "2        1987\n",
       "3        1979\n",
       "4        1979\n",
       "5        1979\n",
       "6        1979\n",
       "7        1978\n",
       "8        1976\n",
       "9        1976\n",
       "10       1976\n",
       "11       1965\n",
       "12       1985\n",
       "13       1976\n",
       "14       1965\n",
       "15       1968\n",
       "16       1968\n",
       "17       1968\n",
       "18       1999\n",
       "19       1968\n",
       "20       1968\n",
       "21       1968\n",
       "22       1968\n",
       "23       1967\n",
       "24       1969\n",
       "25       2001\n",
       "26       2001\n",
       "27       2001\n",
       "28       2001\n",
       "29       2001\n",
       "30       2001\n",
       "         ... \n",
       "11643    2014\n",
       "11644    1991\n",
       "11645    2012\n",
       "11646    2012\n",
       "11647    2013\n",
       "11648    1995\n",
       "11649    2003\n",
       "11650    2000\n",
       "11651    2000\n",
       "11652    1995\n",
       "11653    1998\n",
       "11654    2000\n",
       "11655    1985\n",
       "11656    2001\n",
       "11657    2000\n",
       "11658    2000\n",
       "11659    1997\n",
       "11660    2006\n",
       "11661    1997\n",
       "11662    2000\n",
       "11663    2003\n",
       "11664    1959\n",
       "11665    2006\n",
       "11666    2009\n",
       "11667    2013\n",
       "11668    2012\n",
       "11669    2015\n",
       "11670    2010\n",
       "11671    2013\n",
       "11672    1996\n",
       "Name: year, Length: 11559, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_sets['year'].replace(pd.to_datetime(clean_sets.year).dt.year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The time range is 67 years, between 1950 and 2017.\n"
     ]
    }
   ],
   "source": [
    "# Calculate time range\n",
    "start_year = clean_sets.year.min()\n",
    "end_year = clean_sets.year.max()\n",
    "time_range = int(clean_sets.year.max()) - int(clean_sets.year.min())\n",
    "print('The time range is ' + str(time_range) + \" years, between \" + str(start_year) + \" and \" + str(end_year) + \".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "161796\n",
      "1605586.0\n",
      "% of zero values: 0.10077068434826911\n",
      "2.7670542575540584\n"
     ]
    }
   ],
   "source": [
    "# Cleaning inventory_parts.quantity\n",
    "\n",
    "# Check whether any i_p has an undefined quntity\n",
    "print(inventory_parts.quantity.isna().sum())\n",
    "\n",
    "# Replace all infinite values with 0 to simplify counting them\n",
    "inventory_parts.quantity = inventory_parts.quantity.replace([np.inf, -np.inf], 0.0)\n",
    "\n",
    "# Count amount of values equaling zero\n",
    "quantity_zero = (inventory_parts['quantity'] < 1).sum()\n",
    "print(quantity_zero)\n",
    "print(inventory_parts.quantity.sum())\n",
    "\n",
    "# Calculate % of values equaling zero\n",
    "print(\"% of zero values: \" + str(quantity_zero/inventory_parts.quantity.sum()))\n",
    "\n",
    "print(inventory_parts.quantity.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__\\* Briefly explain your approach here \\*__\n",
    "\n",
    "#### Cleaning sets by the column year\n",
    "1. First check whether all years are defined\n",
    "2. Print years to see examples of inconsistencies in the data. Examples of inconsistencies are negative years, years with five digits and years written as decade+\"s\"(70s, 80s, etc).\n",
    "3. Cleaning:\n",
    "    1. First, all negative years are turned positive.\n",
    "    2. Secondly, all years with 5 digits are turned into 4 digit numbers. We assume that the last digit has been repeated and therefore remove it. For example, 20011 is turned into 2001.\n",
    "    3. Lastly we omit all rows where the year is written as decade+\"s\", as there is no way of knowing in which year of the decade the set is from.\n",
    "    \n",
    "#### Cleaning inventory_parts by the column quantity\n",
    "1. First check whether quantity is defined for each row\n",
    "2. Print quantity to see examples of inconsistencies in the data. An example of an inconsistency is that some of the quantities are written as \"-inf\".\n",
    "3. Cleaning:\n",
    "    1. Set all quantities with the value \"-inf\" to 0.0 instead, as 0.0 is not used in the rest of the array. This way the data will remain but functions such as count, sum etc. can be applied to the column.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A3. Querying phase\n",
    "Answer the following queries using the functionality of `Pandas`:\n",
    "\n",
    "1. List the ids of the inventories that belong to sets that contain cars. (*Hint: Find a smart way to distinguish which sets contain cars based on the sets' name*).\n",
    "* Plot the distribution of part categories as a (horizontal) bar chart. Restrict yourself to the 20 largest part categories (in terms of the number of parts belonging to the category).\n",
    "* Find the dominant color of each set. Then, plot using a (horizontal) bar chart, the number of sets per dominant color. Color each bar with the respective color that it represents.\n",
    "* Create a scatter plot of the *luminance*\\* of the sets vs their publishing year. What do you observe for the years 1980-1981? How do you interpret what you see?\n",
    "\n",
    "\\*The luminance of a color is a [measure of brightness](https://en.wikipedia.org/wiki/Luminance) which, given its RGB representation, can be computed as follows:\n",
    "\n",
    "$luminance = \\sqrt{0.299*R^2 + 0.587*G^2 + 0.114*B^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                                               name  year  \\\n",
      "0   10002-1                                  Railroad Club Car  2001   \n",
      "1   10022-1  Santa Fe Cars - Set II (dining, observation, o...  2002   \n",
      "2   10022-1  Santa Fe Cars - Set II (dining, observation, o...  2002   \n",
      "3   10025-1        Santa Fe Cars - Set I (mail or baggage car)  2002   \n",
      "4   10025-1        Santa Fe Cars - Set I (mail or baggage car)  2002   \n",
      "5   10158-1                               High Speed Train Car  2004   \n",
      "6    1247-1                                         Patrol Car  1999   \n",
      "7    1255-1                                     Shell Car Wash  1999   \n",
      "8    1496-1                                          Rally Car  1987   \n",
      "9    1517-1                                           Race Car  1989   \n",
      "10   1518-1                                    Race Car Repair  1989   \n",
      "11   1741-1                                                Car  1994   \n",
      "12   1990-1                                        F1 Race Car  1993   \n",
      "13   2156-1                                                Car  2000   \n",
      "14   2535-1                               Formula 1 Racing Car  1998   \n",
      "15   2541-1                                    Adventurers Car  1998   \n",
      "16  2824-20  Advent Calendar 2010 City (Day 19) Toy Train C...  2010   \n",
      "17  2824-22  Advent Calendar 2010 City (Day 21) Toy Train C...  2010   \n",
      "18   2886-1                               Formula 1 Racing Car  1997   \n",
      "19   2886-1                               Formula 1 Racing Car  1997   \n",
      "20   2995-1                         Adventurers Car & Skeleton  1998   \n",
      "21   3005-1                        Kabaya Promotional Set: Car  1999   \n",
      "22   3177-1                                          Small Car  2010   \n",
      "23   3177-1                                          Small Car  2010   \n",
      "24    346-2                                     House with Car  1969   \n",
      "25    347-1                        Fire Station with Mini Cars   NaN   \n",
      "26  4024-23     Advent Calendar 2003 Creator (Day 22) Race Car  2003   \n",
      "27   4435-1                                    Car and Caravan  2012   \n",
      "28   4436-1                                         Patrol Car  2012   \n",
      "29   4436-1                                         Patrol Car  2012   \n",
      "..      ...                                                ...   ...   \n",
      "42   7236-2                  Police Car - Blue Sticker Version  2008   \n",
      "43   7236-2                  Police Car - Blue Sticker Version  2008   \n",
      "44   7236-2                  Police Car - Blue Sticker Version  2008   \n",
      "45   7241-1                                           Fire Car  2005   \n",
      "46   7241-1                                           Fire Car  2005   \n",
      "47   7241-1                                           Fire Car  2005   \n",
      "48   7241-1                                           Fire Car  2005   \n",
      "49   7241-1                                           Fire Car  2005   \n",
      "50   7241-1                                           Fire Car  2005   \n",
      "51   7241-1                                           Fire Car  2005   \n",
      "52   7241-1                                           Fire Car  2005   \n",
      "53   7241-1                                           Fire Car  2005   \n",
      "54   7241-1                                           Fire Car  2005   \n",
      "55   7241-1                                           Fire Car  2005   \n",
      "56   7241-1                                           Fire Car  2005   \n",
      "57  7324-20          Advent Calendar 2005 City (Day 19) RC Car  2005   \n",
      "58  75023-6  Advent Calendar 2013, Star Wars (Day  5) - Twi...  2013   \n",
      "59  7553-16  Advent Calendar 2011 City (Day 15) Police Car ...  2011   \n",
      "60  7553-17  Advent Calendar 2011 City (Day 16) Police Car ...  2011   \n",
      "61  7553-18  Advent Calendar 2011 City (Day 17) Police Car ...  2011   \n",
      "62  7687-20  Advent Calendar 2009 City (Day 19) Child and R...  2009   \n",
      "63  7687-21  Advent Calendar 2009 City (Day 20) RC Car Cone...  2009   \n",
      "64   7902-1                                       Doctor's Car  2006   \n",
      "65   7902-1                                       Doctor's Car  2006   \n",
      "66   7902-1                                       Doctor's Car  2006   \n",
      "67  7907-15  Advent Calendar 2007 City (Day 14) Car Wash Kiosk  2007   \n",
      "68   8402-1                                         Sports Car  2009   \n",
      "69   8402-1                                         Sports Car  2009   \n",
      "70  8803-11                     Race Car Driver - Complete Set  2011   \n",
      "71   9678-1                        Twin-pod Cloud Car & Bespin  2012   \n",
      "\n",
      "    theme_id  num_parts  inventory_id   set_id  quantity  \n",
      "0        233        272         13941  10002-1         1  \n",
      "1        237        410         10158  10022-1         1  \n",
      "2        237        410         13083  10022-1         3  \n",
      "3        237        325         10158  10025-1         1  \n",
      "4        237        325         13083  10025-1         2  \n",
      "5        239        151          5358  10158-1         2  \n",
      "6         50         32         13380   1247-1         1  \n",
      "7         99        136          8202   1255-1         1  \n",
      "8         82         52         16131   1496-1         1  \n",
      "9         82         32          1716   1517-1         1  \n",
      "10        82         79          1716   1518-1         1  \n",
      "11        83         36         10406   1741-1         1  \n",
      "12        82         35         14174   1990-1         1  \n",
      "13       467         17          7430   2156-1         1  \n",
      "14        82         30         14711   2535-1         1  \n",
      "15       297         24         14711   2541-1         1  \n",
      "16       220         15         12851  2824-20         1  \n",
      "17       220         15         12851  2824-22         1  \n",
      "18        82         25         10515   2886-1         1  \n",
      "19        82         25         14919   2886-1         1  \n",
      "20       297         69          2814   2995-1         1  \n",
      "21        12         26         15010   3005-1         1  \n",
      "22        63         43          5904   3177-1         1  \n",
      "23        63         43          7151   3177-1         1  \n",
      "24       413        167         11966    346-2         1  \n",
      "25       417        205         11966    347-1         1  \n",
      "26       223         17          8140  4024-23         1  \n",
      "27        63        217          6066   4435-1         1  \n",
      "28        61         96          1649   4436-1         1  \n",
      "29        61         96          6530   4436-1         1  \n",
      "..       ...        ...           ...      ...       ...  \n",
      "42        61         59          6296   7236-2         1  \n",
      "43        61         59         10028   7236-2         1  \n",
      "44        61         59         12110   7236-2         1  \n",
      "45        58         47          1401   7241-1         1  \n",
      "46        58         47          1418   7241-1         1  \n",
      "47        58         47          1732   7241-1         1  \n",
      "48        58         47          3109   7241-1         1  \n",
      "49        58         47          3764   7241-1         1  \n",
      "50        58         47          4430   7241-1         1  \n",
      "51        58         47          4969   7241-1         1  \n",
      "52        58         47          5857   7241-1         1  \n",
      "53        58         47          7151   7241-1         1  \n",
      "54        58         47         11861   7241-1         1  \n",
      "55        58         47         13696   7241-1         1  \n",
      "56        58         47         15694   7241-1         1  \n",
      "57       220          8          7363  7324-20         1  \n",
      "58       225         16          4051  75023-6         1  \n",
      "59       220         14          8820  7553-16         1  \n",
      "60       220         11          8820  7553-17         1  \n",
      "61       220          7          8820  7553-18         1  \n",
      "62       220         12          7881  7687-20         1  \n",
      "63       220         15          7881  7687-21         1  \n",
      "64        60         65          4187   7902-1         1  \n",
      "65        60         65         13355   7902-1         1  \n",
      "66        60         65         13398   7902-1         1  \n",
      "67       220         11          8348  7907-15         1  \n",
      "68        63         68          5904   8402-1         1  \n",
      "69        63         68          9426   8402-1         1  \n",
      "70       538          7         12437  8803-11         1  \n",
      "71       177         78           500   9678-1         1  \n",
      "\n",
      "[72 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "# Get sets with \"car\\s\" in the name\n",
    "set_with_cars = sets[sets.name.str.contains(r'^(?:.*\\s)?cars?(?:\\s.*)?$', flags=re.IGNORECASE)]\n",
    "\n",
    "inventory_cars = pd.merge(set_with_cars, inventory_sets, left_on='id', right_on='set_id')\n",
    "print(inventory_cars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__\\* Briefly explain your approach for every query here \\*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task B. Drop the bike\n",
    "\n",
    "*Los Angeles Metro* has been sharing publicly [anonymized *Metro Bike Share* trip data](https://bikeshare.metro.net/about/data/) under the [Open Database License (ODbL)](http://opendatacommons.org/licenses/odbl/1.0/).\n",
    "\n",
    "In this task you will again perform data wrangling and interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B1. Loading phase\n",
    "Load the json file into a `DataFrame`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "BIKES_DATA_FOLDER = DATA_FOLDER + '/bikes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bike ID</th>\n",
       "      <th>Duration</th>\n",
       "      <th>End Time</th>\n",
       "      <th>Ending Station ID</th>\n",
       "      <th>Ending Station Latitude</th>\n",
       "      <th>Ending Station Longitude</th>\n",
       "      <th>Passholder Type</th>\n",
       "      <th>Plan Duration</th>\n",
       "      <th>Start Time</th>\n",
       "      <th>Starting Station ID</th>\n",
       "      <th>Starting Station Latitude</th>\n",
       "      <th>Starting Station Longitude</th>\n",
       "      <th>Trip ID</th>\n",
       "      <th>Trip Route Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6281.0</td>\n",
       "      <td>180</td>\n",
       "      <td>2016-07-07T04:20:00</td>\n",
       "      <td>3014.0</td>\n",
       "      <td>34.056610</td>\n",
       "      <td>-118.23721</td>\n",
       "      <td>Monthly Pass</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2016-07-07T04:17:00</td>\n",
       "      <td>3014.0</td>\n",
       "      <td>34.056610</td>\n",
       "      <td>-118.23721</td>\n",
       "      <td>1912818</td>\n",
       "      <td>Round Trip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6281.0</td>\n",
       "      <td>1980</td>\n",
       "      <td>2016-07-07T06:33:00</td>\n",
       "      <td>3014.0</td>\n",
       "      <td>34.056610</td>\n",
       "      <td>-118.23721</td>\n",
       "      <td>Monthly Pass</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2016-07-07T06:00:00</td>\n",
       "      <td>3014.0</td>\n",
       "      <td>34.056610</td>\n",
       "      <td>-118.23721</td>\n",
       "      <td>1919661</td>\n",
       "      <td>Round Trip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5861.0</td>\n",
       "      <td>300</td>\n",
       "      <td>2016-07-07T10:37:00</td>\n",
       "      <td>3016.0</td>\n",
       "      <td>34.052898</td>\n",
       "      <td>-118.24156</td>\n",
       "      <td>Flex Pass</td>\n",
       "      <td>365.0</td>\n",
       "      <td>2016-07-07T10:32:00</td>\n",
       "      <td>3016.0</td>\n",
       "      <td>34.052898</td>\n",
       "      <td>-118.24156</td>\n",
       "      <td>1933383</td>\n",
       "      <td>Round Trip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5861.0</td>\n",
       "      <td>10860</td>\n",
       "      <td>2016-07-07T13:38:00</td>\n",
       "      <td>3016.0</td>\n",
       "      <td>34.052898</td>\n",
       "      <td>-118.24156</td>\n",
       "      <td>Flex Pass</td>\n",
       "      <td>365.0</td>\n",
       "      <td>2016-07-07T10:37:00</td>\n",
       "      <td>3016.0</td>\n",
       "      <td>34.052898</td>\n",
       "      <td>-118.24156</td>\n",
       "      <td>1944197</td>\n",
       "      <td>Round Trip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6674.0</td>\n",
       "      <td>420</td>\n",
       "      <td>2016-07-07T12:58:00</td>\n",
       "      <td>3032.0</td>\n",
       "      <td>34.049889</td>\n",
       "      <td>-118.25588</td>\n",
       "      <td>Walk-up</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-07-07T12:51:00</td>\n",
       "      <td>3032.0</td>\n",
       "      <td>34.049889</td>\n",
       "      <td>-118.25588</td>\n",
       "      <td>1940317</td>\n",
       "      <td>Round Trip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6717.0</td>\n",
       "      <td>780</td>\n",
       "      <td>2016-07-07T13:04:00</td>\n",
       "      <td>3054.0</td>\n",
       "      <td>34.039219</td>\n",
       "      <td>-118.23649</td>\n",
       "      <td>Monthly Pass</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2016-07-07T12:51:00</td>\n",
       "      <td>3021.0</td>\n",
       "      <td>34.045609</td>\n",
       "      <td>-118.23703</td>\n",
       "      <td>1944075</td>\n",
       "      <td>One Way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5721.0</td>\n",
       "      <td>600</td>\n",
       "      <td>2016-07-07T13:04:00</td>\n",
       "      <td>3014.0</td>\n",
       "      <td>34.056610</td>\n",
       "      <td>-118.23721</td>\n",
       "      <td>Monthly Pass</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2016-07-07T12:54:00</td>\n",
       "      <td>3022.0</td>\n",
       "      <td>34.046070</td>\n",
       "      <td>-118.23309</td>\n",
       "      <td>1944073</td>\n",
       "      <td>One Way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5957.0</td>\n",
       "      <td>600</td>\n",
       "      <td>2016-07-07T13:09:00</td>\n",
       "      <td>3005.0</td>\n",
       "      <td>34.048550</td>\n",
       "      <td>-118.25905</td>\n",
       "      <td>Flex Pass</td>\n",
       "      <td>365.0</td>\n",
       "      <td>2016-07-07T12:59:00</td>\n",
       "      <td>3076.0</td>\n",
       "      <td>34.040600</td>\n",
       "      <td>-118.25384</td>\n",
       "      <td>1944067</td>\n",
       "      <td>One Way</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6137.0</td>\n",
       "      <td>2880</td>\n",
       "      <td>2016-07-07T13:49:00</td>\n",
       "      <td>3031.0</td>\n",
       "      <td>34.044701</td>\n",
       "      <td>-118.25244</td>\n",
       "      <td>Flex Pass</td>\n",
       "      <td>365.0</td>\n",
       "      <td>2016-07-07T13:01:00</td>\n",
       "      <td>3031.0</td>\n",
       "      <td>34.044701</td>\n",
       "      <td>-118.25244</td>\n",
       "      <td>1944062</td>\n",
       "      <td>Round Trip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6351.0</td>\n",
       "      <td>960</td>\n",
       "      <td>2016-07-07T13:17:00</td>\n",
       "      <td>3078.0</td>\n",
       "      <td>34.064281</td>\n",
       "      <td>-118.23894</td>\n",
       "      <td>Monthly Pass</td>\n",
       "      <td>30.0</td>\n",
       "      <td>2016-07-07T13:01:00</td>\n",
       "      <td>3031.0</td>\n",
       "      <td>34.044701</td>\n",
       "      <td>-118.25244</td>\n",
       "      <td>1944063</td>\n",
       "      <td>One Way</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Bike ID  Duration             End Time  Ending Station ID  \\\n",
       "0   6281.0       180  2016-07-07T04:20:00             3014.0   \n",
       "1   6281.0      1980  2016-07-07T06:33:00             3014.0   \n",
       "2   5861.0       300  2016-07-07T10:37:00             3016.0   \n",
       "3   5861.0     10860  2016-07-07T13:38:00             3016.0   \n",
       "4   6674.0       420  2016-07-07T12:58:00             3032.0   \n",
       "5   6717.0       780  2016-07-07T13:04:00             3054.0   \n",
       "6   5721.0       600  2016-07-07T13:04:00             3014.0   \n",
       "7   5957.0       600  2016-07-07T13:09:00             3005.0   \n",
       "8   6137.0      2880  2016-07-07T13:49:00             3031.0   \n",
       "9   6351.0       960  2016-07-07T13:17:00             3078.0   \n",
       "\n",
       "   Ending Station Latitude  Ending Station Longitude Passholder Type  \\\n",
       "0                34.056610                -118.23721    Monthly Pass   \n",
       "1                34.056610                -118.23721    Monthly Pass   \n",
       "2                34.052898                -118.24156       Flex Pass   \n",
       "3                34.052898                -118.24156       Flex Pass   \n",
       "4                34.049889                -118.25588         Walk-up   \n",
       "5                34.039219                -118.23649    Monthly Pass   \n",
       "6                34.056610                -118.23721    Monthly Pass   \n",
       "7                34.048550                -118.25905       Flex Pass   \n",
       "8                34.044701                -118.25244       Flex Pass   \n",
       "9                34.064281                -118.23894    Monthly Pass   \n",
       "\n",
       "   Plan Duration           Start Time  Starting Station ID  \\\n",
       "0           30.0  2016-07-07T04:17:00               3014.0   \n",
       "1           30.0  2016-07-07T06:00:00               3014.0   \n",
       "2          365.0  2016-07-07T10:32:00               3016.0   \n",
       "3          365.0  2016-07-07T10:37:00               3016.0   \n",
       "4            0.0  2016-07-07T12:51:00               3032.0   \n",
       "5           30.0  2016-07-07T12:51:00               3021.0   \n",
       "6           30.0  2016-07-07T12:54:00               3022.0   \n",
       "7          365.0  2016-07-07T12:59:00               3076.0   \n",
       "8          365.0  2016-07-07T13:01:00               3031.0   \n",
       "9           30.0  2016-07-07T13:01:00               3031.0   \n",
       "\n",
       "   Starting Station Latitude  Starting Station Longitude  Trip ID  \\\n",
       "0                  34.056610                  -118.23721  1912818   \n",
       "1                  34.056610                  -118.23721  1919661   \n",
       "2                  34.052898                  -118.24156  1933383   \n",
       "3                  34.052898                  -118.24156  1944197   \n",
       "4                  34.049889                  -118.25588  1940317   \n",
       "5                  34.045609                  -118.23703  1944075   \n",
       "6                  34.046070                  -118.23309  1944073   \n",
       "7                  34.040600                  -118.25384  1944067   \n",
       "8                  34.044701                  -118.25244  1944062   \n",
       "9                  34.044701                  -118.25244  1944063   \n",
       "\n",
       "  Trip Route Category  \n",
       "0          Round Trip  \n",
       "1          Round Trip  \n",
       "2          Round Trip  \n",
       "3          Round Trip  \n",
       "4          Round Trip  \n",
       "5             One Way  \n",
       "6             One Way  \n",
       "7             One Way  \n",
       "8          Round Trip  \n",
       "9             One Way  "
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write your code here\n",
    "bikes = pd.read_json(BIKES_DATA_FOLDER + '/metro-bike-share-trip-data.json.zip')\n",
    "bikes.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B2. Cleaning phase\n",
    "Describe the type and the value range of each attribute. Indicate and transform the attributes that are `Categorical`. Are there redundant columns in the dataset (i.e., are there columns whose value depends only on the value of another column)? What are the possible pitfalls of having such columns? Reduce *data redundancy* by extracting such columns to separate `DataFrames`. Which of the two formats (the initial one or the one with reduced data redundancy) is more susceptible to inconsistencies? At the end print for each `Dataframe` the *type of each column* and it's *shape*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bike ID:\n",
      " Type: float64,\n",
      " Max: 6728.000000,\n",
      " Min: 1349.000000,\n",
      " Value range: 5379.000000\n",
      "Duration:\n",
      " Type: int64,\n",
      " Max: 86400.000000,\n",
      " Min: 60.000000,\n",
      " Value range: 86340.000000\n",
      "Ending Station ID:\n",
      " Type: float64,\n",
      " Max: 4108.000000,\n",
      " Min: 3000.000000,\n",
      " Value range: 1108.000000\n",
      "Ending Station Latitude:\n",
      " Type: float64,\n",
      " Max: 34.064281,\n",
      " Min: 0.000000,\n",
      " Value range: 34.064281\n",
      "Ending Station Longitude:\n",
      " Type: float64,\n",
      " Max: 0.000000,\n",
      " Min: -118.472832,\n",
      " Value range: 118.472832\n",
      "Plan Duration:\n",
      " Type: float64,\n",
      " Max: 365.000000,\n",
      " Min: 0.000000,\n",
      " Value range: 365.000000\n",
      "Starting Station ID:\n",
      " Type: float64,\n",
      " Max: 4108.000000,\n",
      " Min: 3000.000000,\n",
      " Value range: 1108.000000\n",
      "Starting Station Latitude:\n",
      " Type: float64,\n",
      " Max: 34.064281,\n",
      " Min: 0.000000,\n",
      " Value range: 34.064281\n",
      "Starting Station Longitude:\n",
      " Type: float64,\n",
      " Max: 0.000000,\n",
      " Min: -118.472832,\n",
      " Value range: 118.472832\n",
      "Trip ID:\n",
      " Type: int64,\n",
      " Max: 23794218.000000,\n",
      " Min: 1912818.000000,\n",
      " Value range: 21881400.000000\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1008\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1009\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexpressions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0meval_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1010\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(op, op_str, a, b, use_numexpr, **eval_kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_numexpr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0meval_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_numexpr\u001b[0;34m(op, op_str, a, b, truediv, reversed, **eval_kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b, **eval_kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36msafe_na_op\u001b[0;34m(lvalues, rvalues)\u001b[0m\n\u001b[1;32m   1029\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1030\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1031\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mna_op\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1014\u001b[0m                 \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnotna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0mnotna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1015\u001b[0;31m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1016\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-391-9ad3ddb458d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mbikes_date_max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbikes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'End Time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Start Time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mbikes_date_min\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbikes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'End Time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Start Time'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mbikes_value_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbikes_date_max\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbikes_date_min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m   1064\u001b[0m             \u001b[0mrvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_na_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m         return construct_result(left, result,\n\u001b[1;32m   1068\u001b[0m                                 index=left.index, name=res_name, dtype=None)\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36msafe_na_op\u001b[0;34m(lvalues, rvalues)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m                 return libalgos.arrmap_object(lvalues,\n\u001b[0;32m-> 1034\u001b[0;31m                                               lambda x: op(x, rvalues))\n\u001b[0m\u001b[1;32m   1035\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/algos_common_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.algos.arrmap_object\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1032\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m                 return libalgos.arrmap_object(lvalues,\n\u001b[0;32m-> 1034\u001b[0;31m                                               lambda x: op(x, rvalues))\n\u001b[0m\u001b[1;32m   1035\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "# Check with shape to get amount of columns\n",
    "\n",
    "\n",
    "# Check min and max to get value range\n",
    "bikes_max_min = bikes.describe(include=[np.number]).loc[['min','max']]\n",
    "value_range = bikes_max_min.loc['max'] - bikes_max_min.loc['min']\n",
    "\n",
    "# Print for all numerical values\n",
    "[print(\n",
    "    '{0:1}:\\n Type: {1:1},\\n Max: {2:.6f},\\n Min: {3:.6f},\\n Value range: {4:0.6f}'.\n",
    "    format(\n",
    "        i,\n",
    "        str(bikes.dtypes.loc[i]),\n",
    "        bikes_max_min.loc['max', i],\n",
    "        bikes_max_min.loc['min', i],\n",
    "        value_range[i]\n",
    "    )\n",
    ") for i in bikes_max_min.columns]\n",
    "\n",
    "bikes_date_max = bikes.get(['End Time', 'Start Time']).max()\n",
    "bikes_date_min = bikes.get(['End Time', 'Start Time']).min()\n",
    "bikes_value_range = bikes_date_max - bikes_date_min\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bike ID                                      6728\n",
      "Duration                                    86400\n",
      "End Time                      2017-04-02T10:32:00\n",
      "Ending Station ID                            4108\n",
      "Ending Station Latitude                   34.0643\n",
      "Ending Station Longitude                        0\n",
      "Passholder Type                           Walk-up\n",
      "Plan Duration                                 365\n",
      "Start Time                    2017-03-31T23:45:00\n",
      "Starting Station ID                          4108\n",
      "Starting Station Latitude                 34.0643\n",
      "Starting Station Longitude                      0\n",
      "Trip ID                                  23794218\n",
      "Trip Route Category                    Round Trip\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# The columns 'Trip Route Category' and 'Passholder Type' are objects of string characters\n",
    "# which can be mapped to categories\n",
    "bikes['Trip Route Category'] = bikes['Trip Route Category'].astype('category')\n",
    "bikes['Passholder Type'] = bikes['Passholder Type'].astype('category')\n",
    "print(bikes.max())\n",
    "\n",
    "# For 'Passholder Type', we see that 'Plan Duration' is NaN for the category 'Staff Annual'.\n",
    "# Since the name is annual, we change NaN to 365\n",
    "index_staff_annual = bikes[bikes['Passholder Type'].apply(lambda x: x == 'Staff Annual')].index\n",
    "bikes.loc[index_staff_annual, 'Plan Duration'] = 365\n",
    "\n",
    "# Check if and in that case which categories have remaining NaN values\n",
    "bikes[bikes['Plan Duration'].isnull()]['Passholder Type'].value_counts()\n",
    "\n",
    "# Get index for remaining nan values in category 'Monthly Pass'\n",
    "index_remaining_nan = bikes[bikes['Passholder Type'].apply(lambda x: x == 'Monthly Pass') & \n",
    "                            bikes['Plan Duration'].isnull()].index\n",
    "\n",
    "# Only 'Monthly Pass' has NaN values, change these to 30\n",
    "bikes.loc[index_remaining_nan, 'Plan Duration'] = 30\n",
    "\n",
    "# Drop all rows where latitude (if latitude is missing, so is longitude) or bike id is missing \n",
    "bikes = bikes.dropna(subset=['Ending Station Latitude', 'Starting Station Latitude', 'Bike ID'])\n",
    "\n",
    "# The columns which depends on other columns are 'Duration', 'Plan Duration' and 'Trip Route Category'\n",
    "# sista är för att den kan beräknas med hjälp av att jämföra start och end station id\n",
    "duration = bikes[['Duration']]\n",
    "plan_duration = bikes[['Plan Duration']]\n",
    "t_r_c = bikes[['Trip Route Category']]\n",
    "\n",
    "# bikes = bikes.drop(['Duration', 'Plan Duration', 'Trip Route Category'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also can match the station to their latitude and longitude, which can be saved in a seperate Dataframe\n",
    "# A Station ID can have two different longitudes. We round these values which make them correspond and then drop\n",
    "# the duplicates. The values are sorted and the table is reindexed.\n",
    "stationID = pd.concat(\n",
    "    [bikes[['Ending Station ID', 'Ending Station Latitude', 'Ending Station Longitude']\n",
    "          ].rename(index=str, columns={\"Ending Station ID\": \"Station ID\",\n",
    "                            \"Ending Station Latitude\": \"Latitude\",\n",
    "                            \"Ending Station Longitude\": \"Longitude\" }),\n",
    "     bikes[['Starting Station ID', 'Starting Station Latitude', 'Starting Station Longitude']\n",
    "          ].rename(index=str, columns={\"Starting Station ID\": \"Station ID\",\n",
    "                            \"Starting Station Latitude\": \"Latitude\",\n",
    "                            \"Starting Station Longitude\": \"Longitude\" })],\n",
    "    sort=False).round(5).drop_duplicates().sort_values(by=['Station ID']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop latitude and longitude for end and start in bikes\n",
    "bikes = bikes.drop(['Ending Station Latitude', 'Ending Station Longitude',\n",
    "                    'Starting Station Latitude', 'Starting Station Longitude'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The datatype of each column in bikes:\n",
      "Bike ID                 float64\n",
      "Duration                  int64\n",
      "End Time                 object\n",
      "Ending Station ID       float64\n",
      "Passholder Type        category\n",
      "Plan Duration           float64\n",
      "Start Time               object\n",
      "Starting Station ID     float64\n",
      "Trip ID                   int64\n",
      "Trip Route Category    category\n",
      "dtype: object\n",
      "\n",
      "The shape of bikes is (131327, 10)\n",
      "\n",
      "The datatype of each column in duration:\n",
      "Duration    int64\n",
      "dtype: object\n",
      "\n",
      "The shape of duration is (131327, 1)\n",
      "\n",
      "The datatype of each column in plan_duration:\n",
      "Plan Duration    float64\n",
      "dtype: object\n",
      "\n",
      "The shape of plan_duration is (131327, 1)\n",
      "\n",
      "The datatype of each column in t_r_c:\n",
      "\n",
      "Trip Route Category    category\n",
      "dtype: object\n",
      "\n",
      "The shape of t_r_c is (131327, 1)\n",
      "\n",
      "The datatype of each column in stationID:\n",
      "\n",
      "Station ID    float64\n",
      "Latitude      float64\n",
      "Longitude     float64\n",
      "dtype: object\n",
      "\n",
      "The shape of stationID is (67, 3)\n"
     ]
    }
   ],
   "source": [
    "# Print all frames\n",
    "\n",
    "# bikes\n",
    "print('The datatype of each column in bikes:')\n",
    "print(bikes.dtypes)\n",
    "print(\"\\nThe shape of bikes is\", bikes.shape)\n",
    "\n",
    "# duration\n",
    "print('\\nThe datatype of each column in duration:')\n",
    "print(duration.dtypes)\n",
    "print(\"\\nThe shape of duration is\", duration.shape)\n",
    "\n",
    "# plan_duration\n",
    "print('\\nThe datatype of each column in plan_duration:')\n",
    "print(plan_duration.dtypes)\n",
    "print(\"\\nThe shape of plan_duration is\", plan_duration.shape)\n",
    "\n",
    "# t_r_c\n",
    "print('\\nThe datatype of each column in t_r_c:\\n')\n",
    "print(t_r_c.dtypes)\n",
    "print(\"\\nThe shape of t_r_c is\", t_r_c.shape)\n",
    "\n",
    "# stationID\n",
    "print('\\nThe datatype of each column in stationID:\\n')\n",
    "print(stationID.dtypes)\n",
    "print(\"\\nThe shape of stationID is\", stationID.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__\\* Briefly explain your approach here \\*__\n",
    "\n",
    "#### Check type and value range of each attribute\n",
    "1. First check type and value range of numerical values\n",
    "2. Then do it for object types\n",
    "\n",
    "#### Categorical\n",
    "1. First check which categories are based on string characters\n",
    "2. Then transform these to the Category type\n",
    "\n",
    "#### Data redundancy\n",
    "1. Redundant columns are 'Duration', 'Plan Duration' and 'Trip Route Category', since all these can be calculated from other columns.\n",
    "2. We extract station IDs, latitude and longitude to create a new dataframe where the IDs are matched with the longitudes\n",
    "3. Pitfalls of having redundant columns are that they take up memory space, if data is compromised in that column the entire is dirty without having to be and it is harder for a human to handle and oversee a big dataset.\n",
    "4. The initial one is more susceptible to inconsistencies. For example, if one value of Duration has been accidentally set to an incorrect value in the first dataset it is not sure the user will notice this. If Duration is calculated based on End Time and Start Time, it will give the correct Duration between these times for each row. However this does not help if End Time or Start Time has been compromised. Another issue that might create inconsistencies for the one with reduced data redundancy is if operations are made on the extracted datasets that change their shape/size, indices etc. so the datasets no longer corresponds to the first dataset. Therefore you have to make sure the data still matches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B3. Querying phase\n",
    "Answer the following queries using the functionality of `Pandas`.\n",
    "\n",
    "1. Plot the *distribution* of the number of outgoing trips from each station in a histogram with 20 bins (Hint: each bin describes a range of counts, not stations).\n",
    "* Plot histograms for the *duration* and *trip starting hour in the day* attributes. For both the *duration*  and the *trip starting hour* use *discrete 1-hour intervals*. What do you observe in each plot? What are some popular values in the *duration* plot? Explain the local maxima and the trends you observe on the *trip starting hour* plot based on human behavior.\n",
    "* For each *trip route category*, calculate the proportion of trips by *passholder type* and present your results in *a stacked bar chart with normalized height*.\n",
    "* Considering only trips that begin in the morning hours (before noon), plot in *a single bar chart* the proportion of trips by *passholder type* and *trip route category*. Explain any outliers you observe.\n",
    "* Separate the hours of the day into two intervals that have (approximately) the same number of bikes leaving the stations. For each of the two intervals calculate the proportion of trips by *passholder type* and *trip route category*. Present your results in a `DataFrame` which has a unique, non-composite index. Does the proportion of trips depend on whether it is the first or second hour interval? Would the company have any significant benefit by creating a more complex paying scheme where monthly pass users would pay less in the first interval and (equally) more on the second one? Assume that the number of trips per interval will not change if the scheme changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__\\* Briefly explain your approach for every query here \\*__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
